{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ce86a-9588-46a9-ac6d-73fa50c89bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf746d8b-0235-4169-b486-e10d00d75186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3307c2-4d23-4bd8-9b59-1bbfd3066685",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf7e66-cf6e-4ed2-803b-17bb8809d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e17d-b378-4728-a2ad-01c2541c2d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfa2cd-674d-46f5-a4ce-8228e23ef7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034398ff-54c7-4be0-80e7-9fb1f617f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c78e2d-f7d1-4281-81ad-5e3c946f1a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_schema',\n",
       " '_all_edges',\n",
       " 'add_conditional_edges',\n",
       " 'add_edge',\n",
       " 'add_node',\n",
       " 'add_sequence',\n",
       " 'compile',\n",
       " 'set_conditional_entry_point',\n",
       " 'set_entry_point',\n",
       " 'set_finish_point',\n",
       " 'validate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langgraph.graph.StateGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93066c93-a023-4340-8301-a448001807f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['END',\n",
       " 'Graph',\n",
       " 'MessageGraph',\n",
       " 'MessagesState',\n",
       " 'START',\n",
       " 'StateGraph',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'add_messages',\n",
       " 'graph',\n",
       " 'message',\n",
       " 'state']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langgraph.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05467f57-df2a-4bb6-808e-559d77d5cb87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_edges',\n",
       " 'add_conditional_edges',\n",
       " 'add_edge',\n",
       " 'add_node',\n",
       " 'compile',\n",
       " 'set_conditional_entry_point',\n",
       " 'set_entry_point',\n",
       " 'set_finish_point',\n",
       " 'validate']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langgraph.graph.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67ab2a-7b99-4845-8b92-61894b1d25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197ff90-3189-46f8-b41a-2640de28b6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d84e72-a97f-48f2-81d4-dce04adfb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph\n",
    "from langgraph import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6ca1191-e5f6-412a-b1ab-d44251217d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000019A1B2B7800>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Step 1: Configure OpenAI Client\n",
    "client = openai.OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'\n",
    ")\n",
    "\n",
    "MODEL = \"llama3.2:latest\"\n",
    "\n",
    "# Step 2: Define State\n",
    "class OverallState(TypedDict):\n",
    "    query: str\n",
    "    messages: list[dict]  # To store all interactions\n",
    "    full_query: str\n",
    "    document_context: str\n",
    "    temp_response: str\n",
    "\n",
    "# Step 3: Interpreter NODE\n",
    "def interpreter_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"Interpret the query and classify the intent.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    prompt = f\"\"\"\n",
    "        Your task is to classify the intent of the user query into one of the following categories: [greeting, rag_request].\n",
    "        The final response or classification should be just the word, nothing else.\n",
    "\n",
    "        Intent Definitions:\n",
    "        - greeting: If the Query is simply a greeting from the user.\n",
    "        - rag_request: If the Query is potentially a domain related query.\n",
    "\n",
    "        User Query: {query}\n",
    "        Intent Classifier:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an intent classifier bot.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    gen_response = response.choices[0].message.content\n",
    "    state[\"temp_response\"] = gen_response\n",
    "#    state[\"messages\"].append({\"role\": \"user\", \"content\": query})\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 4: Greeting Response NODE\n",
    "def greeting_response_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"Respond to the greeting from the user.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    prompt = f\"\"\"\n",
    "        You are an AI Assistant Chatbot here to assist users. Respond to the user's greeting \n",
    "        appropriately and ask how you may help them.\n",
    "\n",
    "        User Greeting: {query}\n",
    "        AI Assistant:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    gen_response = response.choices[0].message.content\n",
    "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": gen_response})\n",
    "    state[\"temp_response\"] = gen_response\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 5: Local Document Loader\n",
    "def load_local_documents(folder_path):\n",
    "    \"\"\"Load all PDF files from a folder as documents.\"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(folder_path, file_name))\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    return documents\n",
    "\n",
    "# Step 6: Process and Index Documents\n",
    "def create_document_store(folder_path):\n",
    "    \"\"\"Create a FAISS-based document store from local documents.\"\"\"\n",
    "    documents = load_local_documents(folder_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(texts, embedding_model)\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Initialize document store\n",
    "current_directory = os.getcwd()\n",
    "document_store = create_document_store(folder_path=current_directory)\n",
    "\n",
    "# Step 7: Document Retrieval NODE\n",
    "def retrieve_documents_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"Retrieve documents based on query.\"\"\"\n",
    "    documents = document_store.similarity_search(state[\"query\"], k=5)\n",
    "\n",
    "    # Combine document contents into a single string\n",
    "    document_context = \"\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    state[\"document_context\"] = document_context\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 8: Generate Paths and Consistency Check NODE\n",
    "def generate_multiple_paths_and_check_consistency_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"Generate multiple reasoning paths and aggregate results.\"\"\"\n",
    "    paths = []\n",
    "    query = state[\"query\"]\n",
    "    document_context = state[\"document_context\"]\n",
    "    full_query = state[\"full_query\"]\n",
    "\n",
    "    for i in range(3):  # Generate 3 reasoning paths\n",
    "        prompt = f\"\"\"\n",
    "            Solve the following problem using a unique approach (Reasoning Path {i + 1}):\n",
    "            Problem/Query: {query}\n",
    "            Documents: {document_context}\n",
    "            Chat History: {full_query}\n",
    "            Reasoning Path {i + 1}:\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a reasoning AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        paths.append(response.choices[0].message.content)\n",
    "\n",
    "    # Aggregate reasoning paths\n",
    "    aggregate_prompt = f\"\"\"\n",
    "        Analyze the following reasoning paths and determine the most consistent answer:\n",
    "        Reasoning Paths:\n",
    "        {\"\\n\".join(paths)}\n",
    "\n",
    "        Most consistent answer with criticisms:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a critical reasoning evaluator.\"},\n",
    "            {\"role\": \"user\", \"content\": aggregate_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    state[\"temp_response\"] = response.choices[0].message.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 8.5: Self-Reflection NODE\n",
    "def self_reflection_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"Perform self-reflection on the generated response.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    temp_response = state[\"temp_response\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Problem/Query: {query}\n",
    "        Generated Answer: {temp_response}\n",
    "\n",
    "        Critique and improve the answer by analyzing its accuracy, relevance, and completeness.\n",
    "        Provide the improved answer along with reasoning for changes made:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a self-reflective AI focused on improving responses.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    improved_response = response.choices[0].message.content\n",
    "    state[\"temp_response\"] = improved_response\n",
    "#    state[\"messages\"].append({\"role\": \"assistant\", \"content\": improved_response})\n",
    "\n",
    "    return state\n",
    "\n",
    "# Step 9: Post-Processing NODE\n",
    "def post_processing_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"Post-process the final answer for user presentation.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    final_answer = state[\"temp_response\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Problem/Query: {query}\n",
    "        Raw Answer: {final_answer}\n",
    "\n",
    "        Provide a polished and user-friendly response:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant specializing in response refinement.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    polished_response = response.choices[0].message.content\n",
    "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": polished_response})\n",
    "\n",
    "    #creating full_query\n",
    "    context = \"\\n\".join(\n",
    "        f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in state[\"messages\"]\n",
    "    )\n",
    "    \n",
    "    full_query = f\"{context}\" if context else state['query']\n",
    "    state[\"full_query\"] = full_query\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 10: Define Workflow Using LangGraph (Updated with Self-Reflection Node)\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"interpreter\", interpreter_node)\n",
    "graph.add_node(\"greeting_response\", greeting_response_node)\n",
    "graph.add_node(\"retrieve_documents\", retrieve_documents_node)\n",
    "graph.add_node(\"generate_paths_and_consistency_check\", generate_multiple_paths_and_check_consistency_node)\n",
    "graph.add_node(\"self_reflection\", self_reflection_node)  # New Node\n",
    "graph.add_node(\"post_processing\", post_processing_node)\n",
    "\n",
    "graph.add_edge(START, \"interpreter\")\n",
    "graph.add_conditional_edges(\n",
    "    source=\"interpreter\",\n",
    "    path=lambda state: state[\"temp_response\"],\n",
    "    path_map={\"greeting\": \"greeting_response\", \"rag_request\": \"retrieve_documents\"}\n",
    ")\n",
    "graph.add_edge(\"greeting_response\", END)\n",
    "graph.add_edge(\"retrieve_documents\", \"generate_paths_and_consistency_check\")\n",
    "graph.add_edge(\"generate_paths_and_consistency_check\", \"self_reflection\")  # Add self-reflection here\n",
    "graph.add_edge(\"self_reflection\", \"post_processing\")  # Route through self-reflection\n",
    "graph.add_edge(\"post_processing\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653fec6-3558-45e4-acb5-92d408be0d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "Hello! It's nice to meet you. How can I assist you today? Do you have a specific question, need help with something, or just want to chat? I'm here to listen and provide assistance. What's on your mind?\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " What are transformers?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "**Transformers: A Comprehensive Overview**\n",
      "\n",
      "Transformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP) and computer vision. Introduced in 2017 by Vaswani et al. in their paper \"Attention is All You Need,\" transformers have become a cornerstone of modern deep learning.\n",
      "\n",
      "**How Transformers Work:**\n",
      "\n",
      "Traditional recurrent neural networks (RNNs) struggle with long-range dependencies in input data, as they process information sequentially from left to right. In contrast, transformers use self-attention mechanisms to capture complex relationships between input elements.\n",
      "\n",
      "The core idea behind transformers is to attend to all positions in the input sequence simultaneously and weigh their importance. This allows the model to capture both local and global dependencies in the input data.\n",
      "\n",
      "**Self-Attention Mechanism:**\n",
      "\n",
      "The self-attention mechanism is a key component of transformers. It consists of three main steps:\n",
      "\n",
      "1.  **Query**: The query vector is computed by taking the dot product of the input embedding with a learned weight matrix.\n",
      "2.  **Key**: The key vector is computed by taking the dot product of the input embedding with another learned weight matrix.\n",
      "3.  **Value**: The value vector is computed by taking the dot product of the input embedding with yet another learned weight matrix.\n",
      "\n",
      "The self-attention mechanism then computes a weighted sum of the value vectors based on the query and key vectors.\n",
      "\n",
      "**Benefits Over Traditional RNNs:**\n",
      "\n",
      "Transformers have several benefits over traditional RNNs, including:\n",
      "\n",
      "*   **Parallelization**: Transformers can be parallelized more easily than RNNs, making them faster to train.\n",
      "*   **Long-range dependencies**: Transformers can capture long-range dependencies in input data more effectively than RNNs.\n",
      "*   **Scalability**: Transformers can handle much larger input sequences than RNNs.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Transformers have been widely adopted in various applications, including:\n",
      "\n",
      "*   **Natural Language Processing (NLP)**: Transformers have achieved state-of-the-art results in many NLP tasks, such as machine translation, question answering, and text classification.\n",
      "*   **Computer Vision**: Transformers have also been applied to computer vision tasks, such as image captioning and object detection.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "While transformers have shown great promise, they also have some limitations. For example:\n",
      "\n",
      "*   **Computational complexity**: Transformers can be computationally expensive to train, especially for large input sequences.\n",
      "*   **Overfitting**: Transformers can suffer from overfitting if not regularized properly.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1.  **Provide concrete examples**: Include specific case studies or examples to illustrate how transformers work in practice.\n",
      "2.  **Balance analogy with technical detail**: Use analogies to explain complex concepts, but also provide concrete technical details to support the claims.\n",
      "3.  **Offer a more comprehensive understanding**: Provide a deeper analysis of transformers, including their limitations and potential applications, to offer a more complete understanding of the subject matter.\n",
      "\n",
      "**Real-World Applications:**\n",
      "\n",
      "Transformers have been successfully applied in various real-world scenarios, such as:\n",
      "\n",
      "*   **Language Translation**: Transformers have achieved state-of-the-art results in machine translation tasks, enabling fast and accurate language translation.\n",
      "*   **Text Summarization**: Transformers can be used to summarize long documents into concise summaries, making it easier for users to quickly grasp the main points.\n",
      "\n",
      "**Future Directions:**\n",
      "\n",
      "As transformers continue to evolve, researchers are exploring new applications and techniques to further improve their performance. Some potential areas of research include:\n",
      "\n",
      "*   **Explainability**: Developing methods to explain how transformers make predictions can help improve trust in these models.\n",
      "*   **Efficient Training**: Investigating ways to train transformers more efficiently can lead to faster training times and better performance.\n",
      "\n",
      "By understanding the inner workings of transformers, we can unlock their full potential and harness their power to solve complex problems in NLP and computer vision.\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " What is attention?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "**Understanding Attention in Deep Learning**\n",
      "\n",
      "Attention is a fundamental concept in deep learning, particularly in transformer-based architectures. It refers to the process of focusing on specific parts of an input data sequence that are relevant for making predictions or taking actions. This mechanism allows models to weigh the importance of different input elements when generating outputs.\n",
      "\n",
      "**What is Self-Attention?**\n",
      "\n",
      "Self-attention is a type of attention mechanism that focuses on the relationships between different elements within an input sequence. It consists of three main components:\n",
      "\n",
      "1. **Query (Q)**: The query vector represents the model's understanding of what it wants to focus on.\n",
      "2. **Key (K)**: The key vector represents the model's understanding of what is relevant in the input data.\n",
      "3. **Value (V)**: The value vector represents the importance or relevance of each element in the input sequence.\n",
      "\n",
      "The self-attention mechanism computes a weighted sum of all possible pairs of query and key vectors, which are then used to compute the final output. This process allows models to selectively focus on specific parts of the input data that are relevant for making predictions.\n",
      "\n",
      "**Visualizing Attention**\n",
      "\n",
      "To better understand attention, consider the following analogy: Imagine you're on a scenic hike, and you want to find the most interesting features along the way. You can use your senses (e.g., sight, sound) to focus on specific aspects of your surroundings that catch your attention. Similarly, self-attention mechanisms in transformer models allow them to selectively focus on relevant input elements based on their relationships with other elements.\n",
      "\n",
      "**Real-World Applications**\n",
      "\n",
      "Attention mechanisms have numerous applications in natural language processing, computer vision, and other areas of deep learning. They enable models to:\n",
      "\n",
      "* Focus on specific parts of the input data that are relevant for making predictions\n",
      "* Handle long-range dependencies and contextual information\n",
      "* Improve model performance by selectively weighting different input elements\n",
      "\n",
      "**Benefits of Attention**\n",
      "\n",
      "Attention provides several benefits in transformer models, including:\n",
      "\n",
      "* **Improved accuracy**: By focusing on relevant input elements, attention mechanisms can improve model accuracy and reduce errors.\n",
      "* **Increased efficiency**: Attention allows models to selectively process input data, reducing computational complexity and improving inference speed.\n",
      "* **Better handling of long-range dependencies**: Self-attention mechanisms can capture long-range relationships between input elements, enabling models to better understand complex patterns in data.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, attention is a fundamental concept in deep learning that enables models to focus on specific parts of the input data that are relevant for making predictions. Self-attention mechanisms, in particular, allow models to selectively weigh different input elements based on their relationships with other elements. By understanding how attention works and its benefits, we can design more effective models that better capture complex patterns and relationships in data.\n",
      "\n",
      "**Example Use Cases**\n",
      "\n",
      "* **Natural Language Processing**: Attention mechanisms are widely used in NLP tasks such as language translation, sentiment analysis, and text summarization.\n",
      "* **Computer Vision**: Self-attention mechanisms have been applied to computer vision tasks such as object detection, image segmentation, and visual question answering.\n",
      "* **Speech Recognition**: Attention mechanisms can improve speech recognition accuracy by selectively focusing on relevant audio features.\n",
      "\n",
      "By incorporating attention mechanisms into transformer models, we can unlock new capabilities in deep learning and achieve state-of-the-art performance in a wide range of applications.\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " How are they both related?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "**Understanding Attention Mechanisms in Deep Learning**\n",
      "\n",
      "Attention mechanisms are a fundamental concept in deep learning that enable models to focus on specific parts of the input data that are relevant for making predictions. In this explanation, we will delve into how attention mechanisms work, their applications, benefits, limitations, and potential areas for improvement.\n",
      "\n",
      "**What is Attention in Deep Learning?**\n",
      "\n",
      "Attention is a mechanism that allows models to selectively focus on specific elements within an input sequence. This is particularly useful in tasks such as language translation, sentiment analysis, and text summarization, where the model needs to identify the most relevant information from the input data.\n",
      "\n",
      "**Self-Attention Mechanism**\n",
      "\n",
      "The self-attention mechanism is a type of attention mechanism that focuses on the relationships between different elements within an input sequence. It works by computing the similarity between all pairs of elements using a dot product and weighting them accordingly to produce a weighted sum of the input elements.\n",
      "\n",
      "Here's a simplified example of how the self-attention mechanism works:\n",
      "\n",
      "1.  Compute the similarity between all pairs of elements using a dot product.\n",
      "2.  Weight the similarities using a softmax function to produce a probability distribution over the input elements.\n",
      "3.  Compute the weighted sum of the input elements using the probability distribution.\n",
      "\n",
      "**Visualizing Attention**\n",
      "\n",
      "To help illustrate how attention mechanisms work, consider the following analogy: Imagine you're on a scenic hike, and you want to find the most interesting features along the way. You can use your senses (e.g., sight, sound) to focus on specific aspects of your surroundings that catch your attention.\n",
      "\n",
      "Similarly, self-attention mechanisms in transformer models allow them to selectively focus on relevant input elements based on their relationships with other elements.\n",
      "\n",
      "**Real-World Applications**\n",
      "\n",
      "Attention mechanisms have numerous applications in natural language processing, computer vision, and speech recognition. Here are a few examples:\n",
      "\n",
      "*   **Language Translation**: Attention mechanisms are widely used in language translation tasks to focus on specific words or phrases that are relevant for translation.\n",
      "    *   For example, the Google Translate model uses attention mechanisms to focus on specific words or phrases that are relevant for translation, such as \"hello\" and \"goodbye\".\n",
      "*   **Sentiment Analysis**: Attention mechanisms are used in sentiment analysis tasks to focus on specific features of the input text that are relevant for sentiment classification.\n",
      "    *   For example, a study published in the Journal of Natural Language Processing found that attention mechanisms can improve sentiment analysis accuracy by focusing on specific words or phrases that are relevant for sentiment classification.\n",
      "*   **Text Summarization**: Attention mechanisms are used in text summarization tasks to focus on specific sentences or phrases that are relevant for summary generation.\n",
      "    *   For example, a study published in the Journal of Artificial Intelligence Research found that attention mechanisms can improve text summarization accuracy by focusing on specific sentences or phrases that are relevant for summary generation.\n",
      "\n",
      "**Benefits and Limitations**\n",
      "\n",
      "Attention mechanisms have several benefits, including:\n",
      "\n",
      "*   Improved model accuracy: By focusing on relevant input elements, attention mechanisms can improve model accuracy and reduce errors.\n",
      "*   Increased efficiency: Attention mechanisms allow models to selectively process input data, reducing computational complexity and improving inference speed.\n",
      "\n",
      "However, attention mechanisms also have some limitations, including:\n",
      "\n",
      "*   Handling long-range dependencies: Self-attention mechanisms can struggle with handling long-range dependencies in the input sequence.\n",
      "    *   For example, a study published in the Journal of Machine Learning Research found that self-attention mechanisms can struggle to handle long-range dependencies in text data.\n",
      "*   Improving efficiency: While attention mechanisms can improve inference speed, they can also increase computational complexity if not implemented efficiently.\n",
      "\n",
      "**Potential Areas for Improvement**\n",
      "\n",
      "There are several potential areas for improvement in attention mechanisms, including:\n",
      "\n",
      "*   **Improving efficiency**: Researchers have proposed various techniques to improve the efficiency of self-attention mechanisms, such as using hierarchical attention or parallelizing the computation.\n",
      "    *   For example, a study published in the Journal of Machine Learning Research found that hierarchical attention can improve the efficiency of self-attention mechanisms by reducing computational complexity.\n",
      "*   **Handling long-range dependencies**: Researchers have proposed various techniques to handle long-range dependencies in self-attention mechanisms, such as using graph neural networks or attention-based models.\n",
      "    *   For example, a study published in the Journal of Artificial Intelligence Research found that graph neural networks can improve the ability of self-attention mechanisms to handle long-range dependencies.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Attention mechanisms are a powerful tool for deep learning models, allowing them to selectively focus on relevant input elements and improve accuracy and efficiency. By understanding how attention mechanisms work and their applications, we can gain a deeper understanding of their potential benefits and limitations.\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Run the Workflow\n",
    "def run_workflow(state: OverallState, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute the LangGraph workflow dynamically.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The persistent state object.\n",
    "        query (str): The user's query to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The final assistant response.\n",
    "    \"\"\"\n",
    "    # Update the state with the new query\n",
    "    state[\"query\"] = query\n",
    "    state[\"messages\"].append({\"role\": \"user\", \"content\": query})\n",
    "    \n",
    "    state[\"temp_response\"] = \"\"  # Clear temp response for the new query\n",
    "\n",
    "     # Step 1: Determine intent\n",
    "    state = interpreter_node(state)\n",
    "\n",
    "    if state[\"temp_response\"] == \"greeting\":\n",
    "        # Handle greeting intent\n",
    "        state = greeting_response_node(state)\n",
    "        \n",
    "        return state[\"temp_response\"]\n",
    "\n",
    "    elif state[\"temp_response\"] == \"rag_request\":\n",
    "        # Handle RAG (Retrieval-Augmented Generation) request intent\n",
    "        state = retrieve_documents_node(state)\n",
    "        state = generate_multiple_paths_and_check_consistency_node(state)\n",
    "\n",
    "        # Self-reflection step\n",
    "        state = self_reflection_node(state)\n",
    "\n",
    "        # Post-processing step\n",
    "        state = post_processing_node(state)\n",
    "        \n",
    "        return state[\"messages\"][-1][\"content\"]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected intent classification result.\")\n",
    "\n",
    "\n",
    "\n",
    "# Main Loop\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize state\n",
    "    state = OverallState(\n",
    "        query = \"\",\n",
    "        messages = [],\n",
    "        full_query = \"\",\n",
    "        document_context = \"\",\n",
    "        temp_response = \"\"\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        print(\"=\" * 250)\n",
    "#        print(\"\\nAI Assistant: \\n\")\n",
    "        user_input = input(\"\\nEnter your query (type 'exit' to quit): \\n\")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"\\n\")\n",
    "            print(\"=\" * 250)\n",
    "            print(\"\\nAI Assistant: \\n\")\n",
    "            print(\"Goodbye! \\n\")\n",
    "            break\n",
    "        response = run_workflow(state, user_input)\n",
    "        print(\"\\n\")\n",
    "        print(\"=\" * 250)\n",
    "        print(f\"\\nAI Assistant: \\n\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bad0ad-6ac8-4c25-89c1-6d48041537e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b8435-1d05-466b-95b9-5e6da583dac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297349e0-c1b1-49f6-b1f7-9effd0d55620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10d532-e7c5-44e9-9c45-33193ac425ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfc205-586b-454a-811f-c556d1b7ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "/////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769c455-b5e2-40ce-8648-65021bbf8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a45e7-f57e-4d5e-8ebb-3902b1551f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e48e329-312e-4099-9a14-1303cd82ec12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "Hello! It's nice to meet you. How can I assist you today? Do you have a specific question, need help with something, or just want to chat? I'm here to listen and provide assistance. What's on your mind?\n",
      "\n",
      "Messages: \n",
      "[{'role': 'user', 'content': 'Hello'}, {'role': 'assistant', 'content': \"Hello! It's nice to meet you. How can I assist you today? Do you have a specific question, need help with something, or just want to chat? I'm here to listen and provide assistance. What's on your mind?\"}]\n",
      "\n",
      "Full Query: \n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " What are transformers?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "**Transformers: A Revolutionary Neural Network Architecture for Natural Language Processing**\n",
      "\n",
      "Transformers are a type of neural network architecture that has revolutionized the field of Natural Language Processing (NLP). Introduced in 2017 by Vaswani et al. in their paper \"Attention is All You Need,\" transformers have become a cornerstone of modern NLP applications.\n",
      "\n",
      "**Key Concepts: Understanding Self-Attention**\n",
      "\n",
      "The core innovation behind transformers lies in the self-attention mechanism, which allows the model to weigh the importance of different input elements relative to each other. This is achieved through a set of learnable weights that are computed based on the similarity between different input sequences.\n",
      "\n",
      "**How Transformers Work**\n",
      "\n",
      "Transformers were designed to parallelize computations across multiple attention heads, making them much faster and more efficient than traditional Recurrent Neural Networks (RNNs). Here's an overview of how they work:\n",
      "\n",
      "1. **Input Embeddings**: The input text is first embedded into a high-dimensional space using an embedding layer.\n",
      "2. **Self-Attention Mechanism**: The self-attention mechanism is applied to the input embeddings, allowing the model to weigh the importance of different input elements relative to each other.\n",
      "3. **Output**: The output of the self-attention mechanism is then passed through a feed-forward network (FFN) and another set of linear layers.\n",
      "\n",
      "**Limitations and Challenges**\n",
      "\n",
      "While transformers have shown remarkable success in various NLP tasks, they also come with some limitations:\n",
      "\n",
      "1. **Vulnerability to Adversarial Attacks**: Transformers are vulnerable to adversarial attacks, which can manipulate the input text to cause the model to produce incorrect outputs.\n",
      "2. **Computational Cost**: While transformers are faster than traditional RNNs in some cases, they also require more computational resources and memory.\n",
      "\n",
      "**Applications and Success Stories**\n",
      "\n",
      "Transformers have been widely adopted for various NLP tasks, including:\n",
      "\n",
      "1. **Language Translation**: Transformers have achieved state-of-the-art results on many language translation benchmarks.\n",
      "2. **Text Classification**: Transformers can be used for text classification tasks, such as sentiment analysis and spam detection.\n",
      "3. **Question Answering**: Transformers have also been applied to question answering tasks, where the model needs to identify relevant passages from a large corpus.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Transformers are a powerful neural network architecture that has revolutionized the field of NLP. By understanding how they work and their limitations, we can design more efficient and effective models for various NLP tasks. As research continues to advance, we can expect transformers to play an increasingly important role in shaping the future of NLP applications.\n",
      "\n",
      "Messages: \n",
      "[{'role': 'user', 'content': 'Hello'}, {'role': 'assistant', 'content': \"Hello! It's nice to meet you. How can I assist you today? Do you have a specific question, need help with something, or just want to chat? I'm here to listen and provide assistance. What's on your mind?\"}, {'role': 'user', 'content': 'What are transformers?'}, {'role': 'assistant', 'content': '**Transformers: A Revolutionary Neural Network Architecture for Natural Language Processing**\\n\\nTransformers are a type of neural network architecture that has revolutionized the field of Natural Language Processing (NLP). Introduced in 2017 by Vaswani et al. in their paper \"Attention is All You Need,\" transformers have become a cornerstone of modern NLP applications.\\n\\n**Key Concepts: Understanding Self-Attention**\\n\\nThe core innovation behind transformers lies in the self-attention mechanism, which allows the model to weigh the importance of different input elements relative to each other. This is achieved through a set of learnable weights that are computed based on the similarity between different input sequences.\\n\\n**How Transformers Work**\\n\\nTransformers were designed to parallelize computations across multiple attention heads, making them much faster and more efficient than traditional Recurrent Neural Networks (RNNs). Here\\'s an overview of how they work:\\n\\n1. **Input Embeddings**: The input text is first embedded into a high-dimensional space using an embedding layer.\\n2. **Self-Attention Mechanism**: The self-attention mechanism is applied to the input embeddings, allowing the model to weigh the importance of different input elements relative to each other.\\n3. **Output**: The output of the self-attention mechanism is then passed through a feed-forward network (FFN) and another set of linear layers.\\n\\n**Limitations and Challenges**\\n\\nWhile transformers have shown remarkable success in various NLP tasks, they also come with some limitations:\\n\\n1. **Vulnerability to Adversarial Attacks**: Transformers are vulnerable to adversarial attacks, which can manipulate the input text to cause the model to produce incorrect outputs.\\n2. **Computational Cost**: While transformers are faster than traditional RNNs in some cases, they also require more computational resources and memory.\\n\\n**Applications and Success Stories**\\n\\nTransformers have been widely adopted for various NLP tasks, including:\\n\\n1. **Language Translation**: Transformers have achieved state-of-the-art results on many language translation benchmarks.\\n2. **Text Classification**: Transformers can be used for text classification tasks, such as sentiment analysis and spam detection.\\n3. **Question Answering**: Transformers have also been applied to question answering tasks, where the model needs to identify relevant passages from a large corpus.\\n\\n**Conclusion**\\n\\nTransformers are a powerful neural network architecture that has revolutionized the field of NLP. By understanding how they work and their limitations, we can design more efficient and effective models for various NLP tasks. As research continues to advance, we can expect transformers to play an increasingly important role in shaping the future of NLP applications.'}]\n",
      "\n",
      "Full Query: \n",
      "User: Hello\n",
      "Assistant: Hello! It's nice to meet you. How can I assist you today? Do you have a specific question, need help with something, or just want to chat? I'm here to listen and provide assistance. What's on your mind?\n",
      "User: What are transformers?\n",
      "Assistant: **Transformers: A Revolutionary Neural Network Architecture for Natural Language Processing**\n",
      "\n",
      "Transformers are a type of neural network architecture that has revolutionized the field of Natural Language Processing (NLP). Introduced in 2017 by Vaswani et al. in their paper \"Attention is All You Need,\" transformers have become a cornerstone of modern NLP applications.\n",
      "\n",
      "**Key Concepts: Understanding Self-Attention**\n",
      "\n",
      "The core innovation behind transformers lies in the self-attention mechanism, which allows the model to weigh the importance of different input elements relative to each other. This is achieved through a set of learnable weights that are computed based on the similarity between different input sequences.\n",
      "\n",
      "**How Transformers Work**\n",
      "\n",
      "Transformers were designed to parallelize computations across multiple attention heads, making them much faster and more efficient than traditional Recurrent Neural Networks (RNNs). Here's an overview of how they work:\n",
      "\n",
      "1. **Input Embeddings**: The input text is first embedded into a high-dimensional space using an embedding layer.\n",
      "2. **Self-Attention Mechanism**: The self-attention mechanism is applied to the input embeddings, allowing the model to weigh the importance of different input elements relative to each other.\n",
      "3. **Output**: The output of the self-attention mechanism is then passed through a feed-forward network (FFN) and another set of linear layers.\n",
      "\n",
      "**Limitations and Challenges**\n",
      "\n",
      "While transformers have shown remarkable success in various NLP tasks, they also come with some limitations:\n",
      "\n",
      "1. **Vulnerability to Adversarial Attacks**: Transformers are vulnerable to adversarial attacks, which can manipulate the input text to cause the model to produce incorrect outputs.\n",
      "2. **Computational Cost**: While transformers are faster than traditional RNNs in some cases, they also require more computational resources and memory.\n",
      "\n",
      "**Applications and Success Stories**\n",
      "\n",
      "Transformers have been widely adopted for various NLP tasks, including:\n",
      "\n",
      "1. **Language Translation**: Transformers have achieved state-of-the-art results on many language translation benchmarks.\n",
      "2. **Text Classification**: Transformers can be used for text classification tasks, such as sentiment analysis and spam detection.\n",
      "3. **Question Answering**: Transformers have also been applied to question answering tasks, where the model needs to identify relevant passages from a large corpus.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Transformers are a powerful neural network architecture that has revolutionized the field of NLP. By understanding how they work and their limitations, we can design more efficient and effective models for various NLP tasks. As research continues to advance, we can expect transformers to play an increasingly important role in shaping the future of NLP applications.\n",
      "==========================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your query (type 'exit' to quit): \n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "Goodbye! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Run the Workflow\n",
    "def run_workflow(state: OverallState, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute the LangGraph workflow dynamically.\n",
    "\n",
    "    Args:\n",
    "        state (OverallState): The persistent state object.\n",
    "        query (str): The user's query to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The final assistant response.\n",
    "    \"\"\"\n",
    "    # Update the state with the new query\n",
    "    state[\"query\"] = query\n",
    "    state[\"messages\"].append({\"role\": \"user\", \"content\": query})\n",
    "    state[\"temp_response\"] = \"\"  # Clear temp response for the new query\n",
    "\n",
    "     # Step 1: Determine intent\n",
    "    state = interpreter_node(state)\n",
    "\n",
    "    if state[\"temp_response\"] == \"greeting\":\n",
    "        # Handle greeting intent\n",
    "        state = greeting_response_node(state)\n",
    "        \n",
    "        return state[\"temp_response\"], state[\"messages\"], state[\"full_query\"]\n",
    "\n",
    "    elif state[\"temp_response\"] == \"rag_request\":\n",
    "        # Handle RAG (Retrieval-Augmented Generation) request intent\n",
    "        state = retrieve_documents_node(state)\n",
    "        state = generate_multiple_paths_and_check_consistency_node(state)\n",
    "\n",
    "        # Self-reflection step\n",
    "        state = self_reflection_node(state)\n",
    "\n",
    "        # Post-processing step\n",
    "        state = post_processing_node(state)\n",
    "        \n",
    "        return state[\"messages\"][-1][\"content\"], state[\"messages\"], state[\"full_query\"]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected intent classification result.\")\n",
    "\n",
    "\n",
    "\n",
    "# Main Loop\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize state\n",
    "    state = OverallState(\n",
    "        query=\"\",\n",
    "        messages=[],\n",
    "        full_query=\"\",\n",
    "        document_context=\"\",\n",
    "        temp_response=\"\"\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        print(\"=\" * 250)\n",
    "#        print(\"\\nAI Assistant: \\n\")\n",
    "        user_input = input(\"\\nEnter your query (type 'exit' to quit): \\n\")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"\\n\")\n",
    "            print(\"=\" * 250)\n",
    "            print(\"\\nAI Assistant: \\n\")\n",
    "            print(\"Goodbye! \\n\")\n",
    "            break\n",
    "        response, msgs, FQ = run_workflow(state, user_input)\n",
    "        print(\"\\n\")\n",
    "        print(\"=\" * 250)\n",
    "        print(f\"\\nAI Assistant: \\n\\n{response}\\n\\nMessages: \\n\\n{msgs}\\n\\nFull Query: \\n\\n{FQ}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94911044-5770-4a1b-8cdf-95c6d8c22933",
   "metadata": {},
   "outputs": [],
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b3dc3-79da-4694-8328-70276c5e9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "W Messages & FullQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82ad28-2a93-4ada-a391-5644ab2b7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e3fef31-439b-4081-8974-934c2ea03d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (type 'exit' to quit): \n",
      " What are transformers?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "**What are Transformers?**\n",
      "\n",
      "Transformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP). They were first introduced in 2017 by Vaswani et al. in their paper \"Attention is All You Need.\" In this explanation, we'll delve into what transformers are, how they work, and their key benefits.\n",
      "\n",
      "**What do Transformers Do?**\n",
      "\n",
      "Imagine you're trying to understand a conversation between two people who speak different languages. A traditional approach would be to translate each sentence individually, which can lead to errors and loss of context. Transformers take a different approach by using self-attention mechanisms to weigh the importance of each word in the input sequence relative to every other word.\n",
      "\n",
      "**How do Transformers Work?**\n",
      "\n",
      "The core component of a transformer is called an encoder layer. Each encoder layer consists of two sub-layers: self-attention and feed-forward neural network (FFNN). The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously, weighing their importance relative to each other.\n",
      "\n",
      "For example, consider the following sentence: \"The dog chased the cat.\" A transformer would break this sentence down into individual words and then use self-attention mechanisms to determine how important each word is in relation to every other word. The output would be a weighted sum of the input words, where the weights represent the importance of each word relative to every other word.\n",
      "\n",
      "**Key Benefits of Transformers:**\n",
      "\n",
      "1. **Self-Attention Mechanisms**: Transformers use self-attention mechanisms to capture long-term dependencies in the input sequence, which is particularly useful for NLP tasks.\n",
      "2. **Parallelizability**: While parallelizability is an important aspect of transformers, it's not as critical as their ability to capture long-term dependencies using self-attention mechanisms.\n",
      "\n",
      "**Real-World Applications:**\n",
      "\n",
      "Transformers have been successfully applied to various NLP tasks, including:\n",
      "\n",
      "* Machine translation\n",
      "* Text summarization\n",
      "* Sentiment analysis\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "In conclusion, transformers are a powerful neural network architecture that has revolutionized the field of natural language processing. Their ability to capture long-term dependencies using self-attention mechanisms makes them particularly useful for NLP tasks. By understanding how transformers work and their key benefits, you can unlock the full potential of these models in your own projects.\n",
      "\n",
      "**Additional Resources:**\n",
      "\n",
      "For further learning, we recommend checking out the original paper by Vaswani et al., \"Attention is All You Need,\" as well as online courses and tutorials that cover transformer architecture.\n",
      "\n",
      "Messages: \n",
      "[{'role': 'assistant', 'content': '**What are Transformers?**\\n\\nTransformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP). They were first introduced in 2017 by Vaswani et al. in their paper \"Attention is All You Need.\" In this explanation, we\\'ll delve into what transformers are, how they work, and their key benefits.\\n\\n**What do Transformers Do?**\\n\\nImagine you\\'re trying to understand a conversation between two people who speak different languages. A traditional approach would be to translate each sentence individually, which can lead to errors and loss of context. Transformers take a different approach by using self-attention mechanisms to weigh the importance of each word in the input sequence relative to every other word.\\n\\n**How do Transformers Work?**\\n\\nThe core component of a transformer is called an encoder layer. Each encoder layer consists of two sub-layers: self-attention and feed-forward neural network (FFNN). The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously, weighing their importance relative to each other.\\n\\nFor example, consider the following sentence: \"The dog chased the cat.\" A transformer would break this sentence down into individual words and then use self-attention mechanisms to determine how important each word is in relation to every other word. The output would be a weighted sum of the input words, where the weights represent the importance of each word relative to every other word.\\n\\n**Key Benefits of Transformers:**\\n\\n1. **Self-Attention Mechanisms**: Transformers use self-attention mechanisms to capture long-term dependencies in the input sequence, which is particularly useful for NLP tasks.\\n2. **Parallelizability**: While parallelizability is an important aspect of transformers, it\\'s not as critical as their ability to capture long-term dependencies using self-attention mechanisms.\\n\\n**Real-World Applications:**\\n\\nTransformers have been successfully applied to various NLP tasks, including:\\n\\n* Machine translation\\n* Text summarization\\n* Sentiment analysis\\n\\n**Conclusion:**\\n\\nIn conclusion, transformers are a powerful neural network architecture that has revolutionized the field of natural language processing. Their ability to capture long-term dependencies using self-attention mechanisms makes them particularly useful for NLP tasks. By understanding how transformers work and their key benefits, you can unlock the full potential of these models in your own projects.\\n\\n**Additional Resources:**\\n\\nFor further learning, we recommend checking out the original paper by Vaswani et al., \"Attention is All You Need,\" as well as online courses and tutorials that cover transformer architecture.'}]\n",
      "\n",
      "Full Query: \n",
      "What are transformers?\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (type 'exit' to quit): \n",
      " What is attention?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "**Understanding Attention in Natural Language Processing**\n",
      "\n",
      "Attention is a fundamental concept in Natural Language Processing (NLP) that enables models to focus on specific parts of input data that are relevant for a particular task. In NLP, attention is used to weight the importance of different words or tokens in a sentence or document, allowing models to selectively attend to the most informative regions.\n",
      "\n",
      "**How Attention Works**\n",
      "\n",
      "Imagine you're reading a sentence and want to understand its meaning. You might focus on specific words that are relevant to the context, such as \"the\" or \"dog.\" This is similar to how attention works in NLP. The model uses an attention mechanism to weigh the importance of each word in the input data, allowing it to selectively attend to the most informative regions.\n",
      "\n",
      "**Mathematical Operations**\n",
      "\n",
      "The attention mechanism typically involves calculating attention weights using a combination of linear and non-linear transformations. One common approach is to use a self-attention mechanism, which calculates attention weights based on the similarity between different tokens in the input data. The formula for self-attention is:\n",
      "\n",
      "`Attention Weight = softmax((Query * Key) / sqrt(Dimension))`\n",
      "\n",
      "where `Query`, `Key`, and `Dimension` are learned parameters that capture the relationships between different tokens.\n",
      "\n",
      "**Self-Attention vs. Other Attention Mechanisms**\n",
      "\n",
      "Self-attention is a key innovation in attention mechanisms, as it allows models to attend to all parts of the input data simultaneously. This is in contrast to other attention mechanisms, such as hard attention or soft attention, which focus on specific regions of the input data.\n",
      "\n",
      "**Key Benefits**\n",
      "\n",
      "Attention has several key benefits for NLP tasks, including:\n",
      "\n",
      "* **Improved performance:** Attention enables models to selectively attend to the most informative regions of the input data, leading to improved performance on tasks such as language modeling and machine translation.\n",
      "* **Increased interpretability:** By highlighting the importance of different tokens in the input data, attention provides a more interpretable understanding of how models are making predictions.\n",
      "\n",
      "**Real-World Applications**\n",
      "\n",
      "Attention has numerous real-world applications in NLP, including:\n",
      "\n",
      "* **Language Translation:** Attention is used to selectively attend to specific words or phrases in source and target languages, allowing for more accurate translations.\n",
      "* **Text Summarization:** Attention is used to identify the most important sentences or phrases in a document, enabling the creation of concise summaries.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Attention is a fundamental concept in NLP that enables models to selectively attend to specific parts of input data. By understanding how attention works and its key benefits, we can better appreciate its significance in this field and develop more effective NLP models.\n",
      "\n",
      "Messages: \n",
      "[{'role': 'assistant', 'content': '**Understanding Attention in Natural Language Processing**\\n\\nAttention is a fundamental concept in Natural Language Processing (NLP) that enables models to focus on specific parts of input data that are relevant for a particular task. In NLP, attention is used to weight the importance of different words or tokens in a sentence or document, allowing models to selectively attend to the most informative regions.\\n\\n**How Attention Works**\\n\\nImagine you\\'re reading a sentence and want to understand its meaning. You might focus on specific words that are relevant to the context, such as \"the\" or \"dog.\" This is similar to how attention works in NLP. The model uses an attention mechanism to weigh the importance of each word in the input data, allowing it to selectively attend to the most informative regions.\\n\\n**Mathematical Operations**\\n\\nThe attention mechanism typically involves calculating attention weights using a combination of linear and non-linear transformations. One common approach is to use a self-attention mechanism, which calculates attention weights based on the similarity between different tokens in the input data. The formula for self-attention is:\\n\\n`Attention Weight = softmax((Query * Key) / sqrt(Dimension))`\\n\\nwhere `Query`, `Key`, and `Dimension` are learned parameters that capture the relationships between different tokens.\\n\\n**Self-Attention vs. Other Attention Mechanisms**\\n\\nSelf-attention is a key innovation in attention mechanisms, as it allows models to attend to all parts of the input data simultaneously. This is in contrast to other attention mechanisms, such as hard attention or soft attention, which focus on specific regions of the input data.\\n\\n**Key Benefits**\\n\\nAttention has several key benefits for NLP tasks, including:\\n\\n* **Improved performance:** Attention enables models to selectively attend to the most informative regions of the input data, leading to improved performance on tasks such as language modeling and machine translation.\\n* **Increased interpretability:** By highlighting the importance of different tokens in the input data, attention provides a more interpretable understanding of how models are making predictions.\\n\\n**Real-World Applications**\\n\\nAttention has numerous real-world applications in NLP, including:\\n\\n* **Language Translation:** Attention is used to selectively attend to specific words or phrases in source and target languages, allowing for more accurate translations.\\n* **Text Summarization:** Attention is used to identify the most important sentences or phrases in a document, enabling the creation of concise summaries.\\n\\n**Conclusion**\\n\\nAttention is a fundamental concept in NLP that enables models to selectively attend to specific parts of input data. By understanding how attention works and its key benefits, we can better appreciate its significance in this field and develop more effective NLP models.'}]\n",
      "\n",
      "Full Query: \n",
      "What is attention?\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (type 'exit' to quit): \n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================================================================================================================================================================================\n",
      "\n",
      "AI Assistant: \n",
      "\n",
      "Goodbye! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Run the Workflow\n",
    "def run_workflow(query):\n",
    "    \"\"\"Execute the LangGraph workflow.\"\"\"\n",
    "\n",
    "    # Initialize state\n",
    "    class OverallState(TypedDict):\n",
    "    query: str\n",
    "    messages: list[dict]  # To store all interactions\n",
    "    full_query: str\n",
    "    document_context: str\n",
    "    temp_response: str\n",
    "    \n",
    "    # Step 0: Append Query to Messages\n",
    "    query = query\n",
    "    state[\"messages\"].append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Step 1: Determine intent\n",
    "    state = interpreter_node(state)\n",
    "\n",
    "    if state[\"temp_response\"] == \"greeting\":\n",
    "        # Handle greeting intent\n",
    "        state = greeting_response_node(state)\n",
    "        \n",
    "        return state[\"temp_response\"], state[\"messages\"], state[\"full_query\"]\n",
    "\n",
    "    elif state[\"temp_response\"] == \"rag_request\":\n",
    "        # Handle RAG (Retrieval-Augmented Generation) request intent\n",
    "        state = retrieve_documents_node(state)\n",
    "        state = generate_multiple_paths_and_check_consistency_node(state)\n",
    "\n",
    "        # Self-reflection step\n",
    "        state = self_reflection_node(state)\n",
    "\n",
    "        # Post-processing step\n",
    "        state = post_processing_node(state)\n",
    "        \n",
    "        return state[\"messages\"][-1][\"content\"], state[\"messages\"], state[\"full_query\"]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected intent classification result.\")\n",
    "\n",
    "# Main Loop\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"=\" * 300)\n",
    "        print(\"\\nAI Assistant: \\n\")\n",
    "        user_input = input(\"Enter your query (type 'exit' to quit): \\n\")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"=\" * 300)\n",
    "            print(\"\\nAI Assistant: \\n\")\n",
    "            print(\"Goodbye! \\n\")\n",
    "            break\n",
    "        response, msgs, FQ = run_workflow(user_input)\n",
    "        print(\"=\" * 300)\n",
    "        print(f\"\\nAI Assistant: \\n\\n{response}\\n\\nMessages: \\n{msgs}\\n\\nFull Query: \\n{FQ}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc445008-08dc-4c32-84bb-7c6e858eff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7dd32-328e-44c6-b074-3283c886e327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a99aa-ab0b-4d70-9c4c-9fa1ac6c34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "//////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb2fbe-a3f2-41d3-8617-d66349f4fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033c95d-f57a-4cd2-a54e-ee0aa0670094",
   "metadata": {},
   "outputs": [],
   "source": [
    "//////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c7270-421a-44de-b40f-19149d0d360c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93884d0-4de3-4993-8c3a-e6808047636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60c6b13a-4525-4763-9942-6e3c6861b2c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Transformers: A Paradigm Shift in Natural Language Processing**\n",
      "\n",
      "Transformers represent a groundbreaking innovation in natural language processing (NLP) tasks, revolutionizing the way we approach sequential data. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies due to their sequential nature and limited ability to process input sequences simultaneously.\n",
      "\n",
      "**The Self-Attention Mechanism: A Key Innovation**\n",
      "\n",
      "The core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\n",
      "\n",
      "**Key Benefits of Transformers**\n",
      "\n",
      "1. **Parallelization**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\n",
      "2. **Long-range Dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\n",
      "\n",
      "**Transformer Architecture**\n",
      "\n",
      "The transformer architecture consists of an encoder-decoder structure, where:\n",
      "\n",
      "* The **encoder** takes in a sequence of tokens and outputs a continuous representation.\n",
      "* The **decoder** generates the output sequence using the attention weights computed by the self-attention mechanism.\n",
      "\n",
      "**How Self-Attention Works**\n",
      "\n",
      "1. Compute attention weights between different input elements based on their similarity.\n",
      "2. Use these attention weights to compute the final output for each token in the input sequence.\n",
      "\n",
      "By leveraging self-attention mechanisms, transformers have become a dominant force in NLP tasks, enabling state-of-the-art performance in applications like language translation, text summarization, and question answering.\n"
     ]
    }
   ],
   "source": [
    "state = OverallState(\n",
    "        query=query,\n",
    "        messages=[],\n",
    "        full_query=\"\",\n",
    "        document_context=\"\",\n",
    "        temp_response=\"\"\n",
    "    ) \n",
    "\n",
    "# Step 1: Determine intent\n",
    "state = interpreter_node(state)\n",
    "\n",
    "if state[\"temp_response\"] == \"greeting\":\n",
    "    # Handle greeting intent\n",
    "    state = greeting_response_node(state)\n",
    "        \n",
    "    print(state[\"temp_response\"])\n",
    "\n",
    "elif state[\"temp_response\"] == \"rag_request\":\n",
    "    # Handle RAG (Retrieval-Augmented Generation) request intent\n",
    "    state = retrieve_documents_node(state)\n",
    "    state = generate_multiple_paths_and_check_consistency_node(state)\n",
    "\n",
    "    # Self-reflection step\n",
    "    state = self_reflection_node(state)\n",
    "\n",
    "    # Post-processing step\n",
    "    state = post_processing_node(state)\n",
    "        \n",
    "    print(state[\"messages\"][-1][\"content\"])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "476614d0-ca78-4946-a5d1-60aba2f9b0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What are transformers?'}, {'role': 'assistant', 'content': \"**Critique:**\\n\\nThe original answer provides a good starting point, but it lacks specificity and depth. To improve the answer, we need to address the criticisms mentioned earlier.\\n\\n* The answer is too vague, as it doesn't provide a detailed explanation of how transformers work or their specific architecture.\\n* It relies on prior knowledge of NLP and RNNs, which may not be familiar to everyone.\\n* Self-attention mechanisms are a key component of transformer architectures, but they are not explicitly mentioned.\\n\\n**Improved Answer:**\\n\\nTransformers represent a significant paradigm shift in natural language processing (NLP) tasks. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies in sequential data due to their sequential nature and limited ability to process input sequences simultaneously.\\n\\nThe core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\\n\\nThis allows transformers to:\\n\\n1. **Parallelize computation**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\\n2. **Capture long-range dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\\n\\nThe transformer architecture consists of an encoder-decoder structure, where the encoder takes in a sequence of tokens and outputs a continuous representation, while the decoder generates the output sequence. The self-attention mechanism is used to compute the attention weights between different input elements, which are then used to compute the final output.\\n\\n**Reasoning for changes:**\\n\\n1. **Added specificity**: I provided more details about how transformers work, including the introduction of self-attention mechanisms.\\n2. **Improved clarity**: I rephrased some sentences to make them clearer and easier to understand.\\n3. **Incorporated key concepts**: I explicitly mentioned self-attention mechanisms, which are a crucial component of transformer architectures.\\n\\nOverall, this improved answer provides more depth and specificity about transformers, making it more accurate, relevant, and complete.\"}, {'role': 'assistant', 'content': '**Transformers: A Paradigm Shift in Natural Language Processing**\\n\\nTransformers represent a groundbreaking innovation in natural language processing (NLP) tasks, revolutionizing the way we approach sequential data. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies due to their sequential nature and limited ability to process input sequences simultaneously.\\n\\n**The Self-Attention Mechanism: A Key Innovation**\\n\\nThe core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\\n\\n**Key Benefits of Transformers**\\n\\n1. **Parallelization**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\\n2. **Long-range Dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\\n\\n**Transformer Architecture**\\n\\nThe transformer architecture consists of an encoder-decoder structure, where:\\n\\n* The **encoder** takes in a sequence of tokens and outputs a continuous representation.\\n* The **decoder** generates the output sequence using the attention weights computed by the self-attention mechanism.\\n\\n**How Self-Attention Works**\\n\\n1. Compute attention weights between different input elements based on their similarity.\\n2. Use these attention weights to compute the final output for each token in the input sequence.\\n\\nBy leveraging self-attention mechanisms, transformers have become a dominant force in NLP tasks, enabling state-of-the-art performance in applications like language translation, text summarization, and question answering.'}]\n"
     ]
    }
   ],
   "source": [
    "print(state[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb6d8fcf-6a8e-489a-994c-aef3fd620303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are transformers?', 'messages': [{'role': 'user', 'content': 'What are transformers?'}, {'role': 'assistant', 'content': \"**Critique:**\\n\\nThe original answer provides a good starting point, but it lacks specificity and depth. To improve the answer, we need to address the criticisms mentioned earlier.\\n\\n* The answer is too vague, as it doesn't provide a detailed explanation of how transformers work or their specific architecture.\\n* It relies on prior knowledge of NLP and RNNs, which may not be familiar to everyone.\\n* Self-attention mechanisms are a key component of transformer architectures, but they are not explicitly mentioned.\\n\\n**Improved Answer:**\\n\\nTransformers represent a significant paradigm shift in natural language processing (NLP) tasks. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies in sequential data due to their sequential nature and limited ability to process input sequences simultaneously.\\n\\nThe core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\\n\\nThis allows transformers to:\\n\\n1. **Parallelize computation**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\\n2. **Capture long-range dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\\n\\nThe transformer architecture consists of an encoder-decoder structure, where the encoder takes in a sequence of tokens and outputs a continuous representation, while the decoder generates the output sequence. The self-attention mechanism is used to compute the attention weights between different input elements, which are then used to compute the final output.\\n\\n**Reasoning for changes:**\\n\\n1. **Added specificity**: I provided more details about how transformers work, including the introduction of self-attention mechanisms.\\n2. **Improved clarity**: I rephrased some sentences to make them clearer and easier to understand.\\n3. **Incorporated key concepts**: I explicitly mentioned self-attention mechanisms, which are a crucial component of transformer architectures.\\n\\nOverall, this improved answer provides more depth and specificity about transformers, making it more accurate, relevant, and complete.\"}, {'role': 'assistant', 'content': '**Transformers: A Paradigm Shift in Natural Language Processing**\\n\\nTransformers represent a groundbreaking innovation in natural language processing (NLP) tasks, revolutionizing the way we approach sequential data. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies due to their sequential nature and limited ability to process input sequences simultaneously.\\n\\n**The Self-Attention Mechanism: A Key Innovation**\\n\\nThe core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\\n\\n**Key Benefits of Transformers**\\n\\n1. **Parallelization**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\\n2. **Long-range Dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\\n\\n**Transformer Architecture**\\n\\nThe transformer architecture consists of an encoder-decoder structure, where:\\n\\n* The **encoder** takes in a sequence of tokens and outputs a continuous representation.\\n* The **decoder** generates the output sequence using the attention weights computed by the self-attention mechanism.\\n\\n**How Self-Attention Works**\\n\\n1. Compute attention weights between different input elements based on their similarity.\\n2. Use these attention weights to compute the final output for each token in the input sequence.\\n\\nBy leveraging self-attention mechanisms, transformers have become a dominant force in NLP tasks, enabling state-of-the-art performance in applications like language translation, text summarization, and question answering.'}], 'full_query': 'User: What are transformers?\\nUser: What are transformers?', 'document_context': \"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/371990120\\nUnderstanding Transformers\\nArticle · April 2023\\nCITATIONS\\n0\\nREADS\\n110\\n1 author:\\nAnkit Hanwate\\nIndian Institute of Technology Bombay\\n2 PUBLICATIONS\\xa0\\xa0\\xa00 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Ankit Hanwate on 03 July 2023.\\nThe user has requested enhancement of the downloaded file.\\napplied to a wide range of NLP tasks, achieving state -\\nof-the-art performance on many benchmark datasets. \\nAs the field of NLP continues to grow, it is likely that \\ntransformers will continue to play a central role in the \\ndevelopment of new models and applications. \\n \\nACKNOWLEDGMENT \\nI’m grateful to Prof. Amit Sethi for his constant \\nguidance and support throughout the duration of the \\ncourse. I’m thankful to the professor for giving me a \\nopportunity to work on this project. His methods of\\nexplore how transformers work and how they have \\nreplaced RNNs as the go-to model for NLP tasks. \\nA. Recurrent Neural Networks \\nBefore we start with our description of transformers, \\nit's important to understand RNNs, which were the \\ndominant model for NLP tasks prior to the introduction \\nof transformers. RNNs are neural networks that are \\ndesigned to handle sequential data, such as text. They \\nwork by maintaining a hidden st ate that is updated as\\nearlier, which can prevent them from learning long-term \\ndependencies. Transformers do not have this problem, \\nas they use self -attention instead of recurrence as \\nexplained earlier in the article. \\nE. Applications of Transformers \\nTransformers have been applied to a wide range of NLP \\ntasks. With the increase in the computation power of \\nworking devices in today’s world use of transformers \\nhas increased mainly because of the accuracy it \\nprovided in the tasks performed.  They have been\\nD. Advantages of Transformers over RNNs   \\nTransformers have a really good edge over the \\ntraditional Recurrent neural network for NLP tasks. \\nFirst, they are highly parallelizable, allowing them to \\nprocess input sequences more quickly and efficiently. \\nThis makes them ideal for large-scale NLP tasks, such \\nas language translation. \\nSecond, transformers are able to capture long -term \\ndependencies more effectively than RNNs, thanks to the \\nself-attention mechanism. The self -attention\", 'temp_response': \"**Critique:**\\n\\nThe original answer provides a good starting point, but it lacks specificity and depth. To improve the answer, we need to address the criticisms mentioned earlier.\\n\\n* The answer is too vague, as it doesn't provide a detailed explanation of how transformers work or their specific architecture.\\n* It relies on prior knowledge of NLP and RNNs, which may not be familiar to everyone.\\n* Self-attention mechanisms are a key component of transformer architectures, but they are not explicitly mentioned.\\n\\n**Improved Answer:**\\n\\nTransformers represent a significant paradigm shift in natural language processing (NLP) tasks. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies in sequential data due to their sequential nature and limited ability to process input sequences simultaneously.\\n\\nThe core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\\n\\nThis allows transformers to:\\n\\n1. **Parallelize computation**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\\n2. **Capture long-range dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\\n\\nThe transformer architecture consists of an encoder-decoder structure, where the encoder takes in a sequence of tokens and outputs a continuous representation, while the decoder generates the output sequence. The self-attention mechanism is used to compute the attention weights between different input elements, which are then used to compute the final output.\\n\\n**Reasoning for changes:**\\n\\n1. **Added specificity**: I provided more details about how transformers work, including the introduction of self-attention mechanisms.\\n2. **Improved clarity**: I rephrased some sentences to make them clearer and easier to understand.\\n3. **Incorporated key concepts**: I explicitly mentioned self-attention mechanisms, which are a crucial component of transformer architectures.\\n\\nOverall, this improved answer provides more depth and specificity about transformers, making it more accurate, relevant, and complete.\"}\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c905c426-add4-4440-ae39-3fd38793767a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Critique:**\n",
      "\n",
      "The original answer provides a good starting point, but it lacks specificity and depth. To improve the answer, we need to address the criticisms mentioned earlier.\n",
      "\n",
      "* The answer is too vague, as it doesn't provide a detailed explanation of how transformers work or their specific architecture.\n",
      "* It relies on prior knowledge of NLP and RNNs, which may not be familiar to everyone.\n",
      "* Self-attention mechanisms are a key component of transformer architectures, but they are not explicitly mentioned.\n",
      "\n",
      "**Improved Answer:**\n",
      "\n",
      "Transformers represent a significant paradigm shift in natural language processing (NLP) tasks. They depart from traditional Recurrent Neural Networks (RNNs), which struggled with capturing long-term dependencies in sequential data due to their sequential nature and limited ability to process input sequences simultaneously.\n",
      "\n",
      "The core innovation of transformers lies in the self-attention mechanism, introduced by Vaswani et al. (2017). This mechanism allows the model to weigh the importance of different input elements relative to each other, enabling it to capture complex relationships between tokens in a sequence. Unlike RNNs, which process input sequences sequentially and rely on recurrent connections to maintain context, transformers use self-attention to jointly attend to all positions in an input sequence simultaneously.\n",
      "\n",
      "This allows transformers to:\n",
      "\n",
      "1. **Parallelize computation**: Transformers can process input sequences in parallel, reducing computational complexity and enabling faster inference times.\n",
      "2. **Capture long-range dependencies**: Self-attention mechanisms enable transformers to capture relationships between tokens that are far apart in the input sequence, which is particularly useful for tasks like machine translation and text summarization.\n",
      "\n",
      "The transformer architecture consists of an encoder-decoder structure, where the encoder takes in a sequence of tokens and outputs a continuous representation, while the decoder generates the output sequence. The self-attention mechanism is used to compute the attention weights between different input elements, which are then used to compute the final output.\n",
      "\n",
      "**Reasoning for changes:**\n",
      "\n",
      "1. **Added specificity**: I provided more details about how transformers work, including the introduction of self-attention mechanisms.\n",
      "2. **Improved clarity**: I rephrased some sentences to make them clearer and easier to understand.\n",
      "3. **Incorporated key concepts**: I explicitly mentioned self-attention mechanisms, which are a crucial component of transformer architectures.\n",
      "\n",
      "Overall, this improved answer provides more depth and specificity about transformers, making it more accurate, relevant, and complete.\n"
     ]
    }
   ],
   "source": [
    "print(state[\"temp_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d975a-566b-4077-b814-09eee88cf1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b420c4b-aee3-4ec2-a4d9-7a514211c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920739c-4a24-46c6-b3d1-46ac49bb4681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c7729-4c27-4afe-af18-f0f8fd5c6d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc95264-01cb-4343-8fc4-c69fc7679ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86872249-f113-4539-9ad1-99153aae53f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meditation offers a wide range of benefits for physical, emotional, mental, and spiritual well-being. Some of the most consistent findings include:\\n\\n1.  **Reduced stress and anxiety**: Meditation has been shown to decrease cortisol levels, heart rate, and blood pressure, leading to a sense of calm and relaxation.\\n\\n2.  **Improved sleep quality**: Regular meditation practice can help improve sleep duration, quality, and depth, resulting in better rest and recovery.\\n\\n3.  **Increased focus and productivity**: Meditation can improve attention, concentration, and mental clarity, making it easier to stay focused and productive throughout the day.\\n\\n4.  **Enhanced emotional regulation**: Meditation helps develop emotional awareness, allowing individuals to better manage their emotions and respond to challenging situations more effectively.\\n\\n5.  **Boosted mood and reduced symptoms of depression**: Meditation has been shown to increase feelings of happiness, calmness, and life satisfaction, while reducing symptoms of depression and anxiety.\\n\\n6.  **Improved immune function**: Research has shown that regular meditation practice can strengthen the immune system, leading to fewer illnesses and a lower risk of chronic diseases.\\n\\n7.  **Increased gray matter and reduced age-related cognitive decline**: Meditation has been shown to increase gray matter in areas of the brain associated with attention, emotion regulation, and memory, while reducing age-related cognitive decline.\\n\\n8.  **Enhanced creativity and problem-solving skills**: Meditation can improve creativity, imagination, and problem-solving skills by increasing access to the subconscious mind and promoting novel connections between ideas.\\n\\n9.  **Better relationships and communication skills**: Meditation can improve emotional intelligence, empathy, and communication skills by increasing self-awareness and allowing individuals to respond more effectively to others.\\n\\n10. **Increased sense of purpose and meaning**: Meditation can increase feelings of purpose, meaning, and fulfillment by promoting self-awareness and helping individuals connect with their values and goals.\\n\\nDifferent types of meditation techniques include:\\n\\n*   Mindfulness Meditation\\n*   Loving-Kindness Meditation\\n*   Transcendental Meditation\\n\\nMeditation has various real-life applications in healthcare, education, and business. It is increasingly being used as a complementary therapy for managing chronic pain, anxiety, depression, and stress-related disorders.\\n\\nOverall, meditation offers numerous benefits that can enhance physical, emotional, mental, and spiritual well-being.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d32e60c-af90-413e-a952-0b5a013c1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"With the help of meditation, what kind of illnesses or conditions can be assuaged? Give me a list of 10-12 such illnesses/conditions.\"\n",
    "full_query, retrieved_documents = retrieve_documents_node(query)\n",
    "improved_result = generate_multiple_paths_and_check_consistency_node(query, full_query, retrieved_documents)\n",
    "self_reflection = self_reflection_node()\n",
    "refined_result = self_reflection.run(query, improved_result)\n",
    "final_response = post_processing_node(query, refined_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f38b4-b007-4bb0-a67c-260d7a1eedbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e697ec4-f895-4e82-ac0c-60b11eb0cced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56abbf9e-2203-4b09-a4de-80af5dc714a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieved_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retrieved_documents\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieved_documents' is not defined"
     ]
    }
   ],
   "source": [
    "retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28be91b1-6ab5-45f8-b8d6-9771f3790c7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m full_query\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_query' is not defined"
     ]
    }
   ],
   "source": [
    "full_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b51593b5-6517-4352-a8ca-aed211f01c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most consistent answer among the provided reasoning paths is the third one:\\n\\nBy embracing meditation as a garden therapy, you can cultivate a thriving landscape of physical, emotional, and spiritual well-being.\\n\\nThis answer is the most consistent because it:\\n\\n1. Aligns with the overall theme of using meditation to promote overall well-being.\\n2. Uses a clear and relatable metaphor (gardening) to describe the benefits of meditation.\\n3. Lists specific illnesses or conditions that can be alleviated or managed through meditation, which is in line with the second reasoning path.\\n4. Provides a unique approach by framing meditation as a garden therapy, rather than solely focusing on stress reduction.\\n\\nThe other two paths have some discrepancies:\\n\\nPath 1 has a more general and somewhat vague tone, while Path 2 is more specific and focused on emotional and psychological aspects of illness.\\n\\nTherefore, the third answer is the most consistent because it offers a clear and relatable framework for understanding the benefits of meditation, while also being mindful of the potential limitations and complexities of using meditation as a therapeutic approach.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f0b77a5-39a3-4d11-9ae8-9f2d39b5f2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The text presents a list of potential illnesses or conditions that may be assuaged with meditation. After analyzing the reasoning paths provided, it appears that the most consistent answer is:\\n\\n1.  **Anxiety Disorders**: Regular meditation practice can reduce symptoms of anxiety by reducing cortisol levels, promoting relaxation response, and enhancing emotional regulation.\\n2.  **Depression**: Meditation improves mood, reduces symptoms, and enhances overall mental health in individuals with depression by increasing production of neurotransmitters like serotonin and dopamine.\\n3.  **Insomnia**: Meditation regulates sleep patterns, improves sleep quality, and increases sleep duration by regulating the body's internal clock and reducing stress and anxiety.\\n4.  **Fibromyalgia**: Meditation reduces chronic pain by promoting relaxation response, enhancing pain tolerance through neural plasticity, and reducing inflammation.\\n5.  **Migraine Management**: Regular meditation practice reduces frequency and severity of migraines by reducing stress and anxiety, promoting relaxation response, and reducing muscle tension.\\n\\nHowever, there are some discrepancies in the text that need to be addressed:\\n\\n*   The list of potential illnesses or conditions has been expanded to include more than 10 conditions. While some of these conditions (such as anxiety disorders, depression, insomnia, fibromyalgia, and migraine management) have a clear connection to meditation benefits, others (such as allergies, asthma, chronic pain management, stress relief, binge eating, cancer, heart disease, high blood pressure, fatigue, substance abuse, and sleep problems) may require more specific evidence or context.\\n*   The text mentions different types of meditation techniques (mindfulness, loving-kindness, and transcendental), but it does not provide information on the most effective type for each condition. This could be an area of further research or exploration.\\n*   While the text emphasizes the importance of consulting a healthcare professional before using meditation as a complementary therapy, it does not provide guidance on how to incorporate meditation into a treatment plan or what specific benefits can be expected from regular practice.\\n\\nTo improve the consistency and accuracy of the answer, the following steps could be taken:\\n\\n1.  **Conduct further research**: Gather more evidence-based information on the effectiveness of meditation for each condition mentioned in the list.\\n2.  **Provide context and clarification**: Offer additional guidance on how to incorporate meditation into a treatment plan and what specific benefits can be expected from regular practice.\\n3.  **Emphasize individual variability**: Acknowledge that each person's experience with meditation can vary significantly due to factors such as prior knowledge, practice consistency, and individual health conditions.\\n\\nBy addressing these discrepancies and providing more detailed information, the text can present a more comprehensive and consistent answer on the potential benefits of meditation for various illnesses or conditions.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45aa9f35-d9d2-46e3-aa9d-208a5969a2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meditation has been extensively researched for its potential benefits in alleviating various illnesses and conditions. While individual results may vary, a consistent practice can lead to significant improvements in overall well-being.\\n\\nSome of the most notable conditions that have shown promise with meditation include:\\n\\n1.  **Anxiety Disorders**: Regular meditation practice has been shown to reduce symptoms of anxiety by reducing cortisol levels, promoting relaxation response, and enhancing emotional regulation. A study published in the Journal of Anxiety Disorders found that mindfulness-based stress reduction (MBSR) significantly reduced symptoms of anxiety in patients with anxiety disorders.\\n\\n2.  **Depression**: Meditation improves mood, reduces symptoms, and enhances overall mental health in individuals with depression by increasing production of neurotransmitters like serotonin and dopamine. A meta-analysis published in the Journal of Psychosomatic Research found that mindfulness-based cognitive therapy (MBCT) significantly reduced symptoms of depression in patients with a history of recurrent depression.\\n\\n3.  **Insomnia**: Meditation regulates sleep patterns, improves sleep quality, and increases sleep duration by regulating the body's internal clock and reducing stress and anxiety. A study published in the Journal of Clinical Sleep Medicine found that mindfulness-based stress reduction (MBSR) significantly improved sleep quality in patients with insomnia.\\n\\n4.  **Fibromyalgia**: Meditation reduces chronic pain by promoting relaxation response, enhancing pain tolerance through neural plasticity, and reducing inflammation. A study published in the Journal of Pain Research found that mindfulness-based stress reduction (MBSR) significantly reduced pain levels in patients with fibromyalgia.\\n\\n5.  **Allergies**: While meditation has been shown to reduce symptoms of anxiety and depression in individuals with allergies, further research is needed to determine its specific effects on allergy management. A study published in the Journal of Alternative and Complementary Medicine found that mindfulness-based stress reduction (MBSR) significantly reduced symptoms of allergic rhinitis in patients with seasonal allergies.\\n\\n6.  **Asthma**: Meditation may help reduce stress and anxiety related to asthma, but more evidence is required to confirm its effectiveness in managing asthma symptoms. A study published in the Journal of Clinical Sleep Medicine found that mindfulness-based stress reduction (MBSR) significantly reduced symptoms of asthma in patients with chronic obstructive pulmonary disease (COPD).\\n\\n7.  **Post-Traumatic Stress Disorder (PTSD)**: Meditation has been shown to reduce symptoms of PTSD by promoting relaxation response, enhancing emotional regulation, and reducing hyperarousal. A study published in the Journal of Clinical Psychology found that mindfulness-based stress reduction (MBSR) significantly reduced symptoms of PTSD in patients with combat-related PTSD.\\n\\n8.  **Eating Disorders**: Meditation improves body image, reduces symptoms, and enhances overall mental health in individuals with eating disorders by increasing self-esteem, promoting positive body image, and reducing obsessive thoughts. A study published in the Journal of Clinical Psychology found that mindfulness-based cognitive therapy (MBCT) significantly reduced symptoms of bulimia nervosa in patients with a history of binge-purge episodes.\\n\\n9.  **Addiction**: Meditation reduces cravings, promotes recovery, and enhances overall mental health in individuals with addiction by increasing self-awareness, promoting emotional regulation, and reducing stress and anxiety. A study published in the Journal of Substance Abuse Treatment found that mindfulness-based relapse prevention (MBRP) significantly reduced recidivism rates in patients with substance use disorders.\\n\\n10. **Chronic Pain**: Meditation reduces chronic pain by promoting relaxation response, enhancing pain tolerance through neural plasticity, and reducing inflammation. A study published in the Journal of Pain Research found that mindfulness-based stress reduction (MBSR) significantly reduced pain levels in patients with chronic back pain.\\n\\n11. **Gastrointestinal Disorders**: Meditation improves gut health, reduces symptoms, and enhances overall mental health in individuals with gastrointestinal disorders by increasing gut motility, reducing inflammation, and promoting positive gut-brain axis interactions. A study published in the Journal of Clinical Gastroenterology found that mindfulness-based stress reduction (MBSR) significantly improved symptoms of irritable bowel syndrome (IBS).\\n\\n12. **Autism Spectrum Disorder**: Meditation improves social skills, reduces symptoms, and enhances overall mental health in individuals with autism spectrum disorder by increasing self-awareness, promoting emotional regulation, and reducing anxiety and stress. A study published in the Journal of Autism and Developmental Disorders found that mindfulness-based cognitive therapy (MBCT) significantly improved social skills and reduced symptoms of ADHD in patients with autism spectrum disorder.\\n\\nTo incorporate meditation into a treatment plan, it's essential to consider the following:\\n\\n1.  **Consult a healthcare professional**: Before starting a meditation practice, consult with a healthcare professional to discuss potential benefits and risks.\\n2.  **Choose the right type of meditation**: Select a meditation technique that aligns with your goals and preferences, such as mindfulness, loving-kindness, or transcendental meditation.\\n3.  **Practice regularly**: Establish a consistent meditation practice to experience its full benefits.\\n4.  **Combine with other therapies**: Consider combining meditation with other therapies, such as cognitive-behavioral therapy (CBT) or physical therapy, for optimal results.\\n\\nIndividuals may experience varying levels of success with meditation due to factors such as:\\n\\n1.  **Prior knowledge and experience**: Those with prior knowledge of meditation or mindfulness practices may experience greater benefits.\\n2.  **Practice consistency**: Regular practice is essential to experiencing the full benefits of meditation.\\n3.  **Individual health conditions**: Meditation's effectiveness can be influenced by individual health conditions, such as chronic pain or mental health disorders.\\n\\nBy acknowledging these limitations and providing more detailed information, it's possible to offer a more comprehensive and accurate answer to the original problem/query.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c49952f3-f729-430d-aba5-18bd969e4ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meditation has been widely recognized as a powerful tool for promoting physical, emotional, and spiritual well-being. By cultivating mindfulness, relaxation, and self-awareness, meditation can help alleviate symptoms of various illnesses and conditions. Here are 12 examples of how meditation can be used therapeutically to manage specific health concerns:\\n\\n1. **Anxiety Disorders**: Meditation reduces symptoms of anxiety by promoting relaxation, reducing rumination, and increasing self-awareness.\\n2. **Chronic Pain**: Meditation decreases chronic pain by activating the parasympathetic nervous system and increasing endorphin production.\\n3. **Depression**: Regular meditation practice reduces symptoms of depression by increasing neurotransmitters like serotonin and dopamine, as well as improving sleep quality.\\n4. **Insomnia**: Guided imagery and progressive muscle relaxation techniques help regulate sleep patterns and improve sleep quality.\\n5. **Post-Traumatic Stress Disorder (PTSD)**: Mindfulness-based stress reduction reduces symptoms of PTSD, including flashbacks and nightmares.\\n6. **Chronic Fatigue Syndrome**: Meditation reduces symptoms of chronic fatigue syndrome, including fatigue, pain, and sleep disturbances.\\n7. **Attention Deficit Hyperactivity Disorder (ADHD)**: Mindfulness meditation improves attention, reduces impulsivity, and enhances executive function skills.\\n8. **Arthritis**: Meditation reduces arthritis pain and inflammation by promoting relaxation and reducing stress.\\n9. **Diabetes**: Regular meditation practice reduces symptoms of diabetes, including blood sugar levels and inflammation.\\n10. **Cancer**: Meditation reduces cancer symptoms, such as pain, fatigue, and anxiety, and improves quality of life.\\n11. **Hypertension**: Mindfulness-based stress reduction lowers blood pressure and improves cardiovascular health.\\n12. **Stress Relief**: Meditation reduces stress by promoting relaxation, reducing rumination, and increasing self-awareness.\\n\\nTo optimize meditation practices:\\n\\n* Aim to meditate at least 3-4 times per week\\n* Start with short sessions (10-20 minutes) and gradually increase duration as you become more comfortable with the practice\\n* Experiment with different types of meditation to find what works best for you\\n* Work with a qualified instructor or healthcare professional to develop a personalized meditation plan\\n\\nBy incorporating mindfulness, relaxation, and self-awareness into our daily lives through meditation, we can reduce symptoms of anxiety, depression, chronic pain, and other health concerns. Regular practice leads to improved sleep quality, cardiovascular health, and overall well-being.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb279c2f-3852-4686-92c9-f5995574b023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
